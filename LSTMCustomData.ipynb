{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"sentiment labelled sentences/sentiment.txt\") as f:\n",
    "#     reviews = f.read()\n",
    "    \n",
    "# data = pd.DataFrame([review.split('\\t') for review in reviews.split('\\n')])\n",
    "\n",
    "# data.columns = ['Review','Sentiment']\n",
    "\n",
    "# data = data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('financial.csv', encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                            Comment\n",
       "0      1  According to Gran , the company has no plans t...\n",
       "1      1  Technopolis plans to develop in stages an area...\n",
       "2      0  The international electronic industry company ...\n",
       "3      2  With the new production plant the company woul...\n",
       "4      2  According to the company 's updated strategy f..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['according',\n",
       " 'to',\n",
       " 'gran',\n",
       " 'the',\n",
       " 'company',\n",
       " 'has',\n",
       " 'no',\n",
       " 'plans',\n",
       " 'to',\n",
       " 'move',\n",
       " 'all',\n",
       " 'production',\n",
       " 'to',\n",
       " 'russia',\n",
       " 'although',\n",
       " 'that',\n",
       " 'is',\n",
       " 'where',\n",
       " 'the',\n",
       " 'company',\n",
       " 'is',\n",
       " 'growing']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_words_reviews(data):\n",
    "    text = list(data['Comment'].values)\n",
    "    clean_text = []\n",
    "    for t in text:\n",
    "        clean_text.append(t.translate(str.maketrans('', '', punctuation)).lower().rstrip())\n",
    "    tokenized = [word_tokenize(x) for x in clean_text]\n",
    "    all_text = []\n",
    "    for tokens in tokenized:\n",
    "        for t in tokens:\n",
    "            all_text.append(t)\n",
    "    return tokenized, set(all_text)\n",
    "\n",
    "reviews, vocab = split_words_reviews(data)\n",
    "\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'combination',\n",
       " 2: 'use',\n",
       " 3: 'got',\n",
       " 4: 'eur111',\n",
       " 5: 'sofa',\n",
       " 6: 'drinking',\n",
       " 7: 'bark',\n",
       " 8: 'updated',\n",
       " 9: 'grodno',\n",
       " 10: 'telpak',\n",
       " 11: 'highrises',\n",
       " 12: 'identification',\n",
       " 13: 'berling',\n",
       " 14: 'weaknesses',\n",
       " 15: 'chains',\n",
       " 16: 'learns',\n",
       " 17: '3931',\n",
       " 18: 'definitive',\n",
       " 19: 'initiatives',\n",
       " 20: 'giant',\n",
       " 21: 'reliable',\n",
       " 22: 'favourably',\n",
       " 23: 'getinge',\n",
       " 24: 'reaching',\n",
       " 25: 'targeted',\n",
       " 26: 'such',\n",
       " 27: 'leminen',\n",
       " 28: 'decisionmaking',\n",
       " 29: 'assessing',\n",
       " 30: '42',\n",
       " 31: 'pedestrian',\n",
       " 32: '360yearold',\n",
       " 33: '1437',\n",
       " 34: 'notified',\n",
       " 35: '04012006',\n",
       " 36: 'supplier',\n",
       " 37: 'sentera',\n",
       " 38: 'online',\n",
       " 39: '527',\n",
       " 40: 'ystok',\n",
       " 41: 'pure',\n",
       " 42: 'worldleading',\n",
       " 43: 'hydrocopper',\n",
       " 44: 'sustainability',\n",
       " 45: 'gather',\n",
       " 46: '2003',\n",
       " 47: '296',\n",
       " 48: 'painful',\n",
       " 49: 'funds',\n",
       " 50: 'leed',\n",
       " 51: 'movements',\n",
       " 52: 'unstable',\n",
       " 53: 'repayment',\n",
       " 54: 'stylised',\n",
       " 55: '48012',\n",
       " 56: 'associations',\n",
       " 57: 'shall',\n",
       " 58: 'process',\n",
       " 59: 'motorcyclist',\n",
       " 60: 'cooperation',\n",
       " 61: 'impacts',\n",
       " 62: 'later',\n",
       " 63: 'vauramo',\n",
       " 64: 'renewing',\n",
       " 65: 'yearearlier',\n",
       " 66: 'leipurin',\n",
       " 67: 'navteq',\n",
       " 68: 'jeder',\n",
       " 69: 'bnpp',\n",
       " 70: 'payphones',\n",
       " 71: '1441',\n",
       " 72: 'fancy',\n",
       " 73: 'extension',\n",
       " 74: 'wholesaler',\n",
       " 75: 'manty',\n",
       " 76: 'oneworld',\n",
       " 77: 'clients',\n",
       " 78: 'ss',\n",
       " 79: 'dongguan',\n",
       " 80: '5500',\n",
       " 81: 'washington',\n",
       " 82: 'nokiabranded',\n",
       " 83: 'partition',\n",
       " 84: 'linkspans',\n",
       " 85: 'eur7',\n",
       " 86: '3378',\n",
       " 87: 'pershare',\n",
       " 88: 'east',\n",
       " 89: 'mittal',\n",
       " 90: 'personnel',\n",
       " 91: 'tanqia',\n",
       " 92: 'kai',\n",
       " 93: 'soyoats',\n",
       " 94: 'coating',\n",
       " 95: 'coker',\n",
       " 96: 'area',\n",
       " 97: '103',\n",
       " 98: 'proud',\n",
       " 99: 'newcomers',\n",
       " 100: 'chairman',\n",
       " 101: 'toppled',\n",
       " 102: 'competitor',\n",
       " 103: '7739',\n",
       " 104: 'cereals',\n",
       " 105: 'goodwill',\n",
       " 106: 'suomussalmi',\n",
       " 107: 'tallon',\n",
       " 108: 'eq',\n",
       " 109: 'file',\n",
       " 110: 'world',\n",
       " 111: 'saavalainen',\n",
       " 112: 'spokesperson',\n",
       " 113: 'phosphorous',\n",
       " 114: 'trail',\n",
       " 115: 'shad',\n",
       " 116: 'rautalinko',\n",
       " 117: 'deutsche',\n",
       " 118: 'how',\n",
       " 119: 'cf2',\n",
       " 120: 'czechrepublic',\n",
       " 121: 'jvc',\n",
       " 122: '395000',\n",
       " 123: 'inhibitor',\n",
       " 124: 'delivery',\n",
       " 125: 'granted',\n",
       " 126: 'replumbing',\n",
       " 127: 'diagnostics',\n",
       " 128: 'incap',\n",
       " 129: 'bordeaux',\n",
       " 130: 'comprehensive',\n",
       " 131: 'commissioned',\n",
       " 132: 'alfa',\n",
       " 133: 'bullish',\n",
       " 134: 'layoffs',\n",
       " 135: 'stability',\n",
       " 136: 'code',\n",
       " 137: 'wwwseahawkdrillingcom',\n",
       " 138: '1520',\n",
       " 139: 'm3',\n",
       " 140: 'mitsubishi',\n",
       " 141: 'focusing',\n",
       " 142: 'elevator',\n",
       " 143: '579000',\n",
       " 144: 'signs',\n",
       " 145: 'loviisa',\n",
       " 146: 'newswires',\n",
       " 147: '1451',\n",
       " 148: 'dct',\n",
       " 149: 'carriers',\n",
       " 150: 'lutterworth',\n",
       " 151: 'humans',\n",
       " 152: 'alteams',\n",
       " 153: 'oversee',\n",
       " 154: 'kiran',\n",
       " 155: '067',\n",
       " 156: 'bands',\n",
       " 157: 'avestapolarit',\n",
       " 158: 'senior',\n",
       " 159: 'usko',\n",
       " 160: 'niina',\n",
       " 161: 'weight',\n",
       " 162: 'acknowledged',\n",
       " 163: 'linde',\n",
       " 164: 'drought',\n",
       " 165: 'therefore',\n",
       " 166: 'ðl',\n",
       " 167: 'seppænen',\n",
       " 168: 'offshore',\n",
       " 169: 'entering',\n",
       " 170: 'hoist',\n",
       " 171: 'flying',\n",
       " 172: 'brief',\n",
       " 173: 'denies',\n",
       " 174: 'etth',\n",
       " 175: 'threatening',\n",
       " 176: '2004b',\n",
       " 177: 'illustration',\n",
       " 178: 'opened',\n",
       " 179: 'portable',\n",
       " 180: 'primarily',\n",
       " 181: 'lp',\n",
       " 182: 'large',\n",
       " 183: 'percentage',\n",
       " 184: '475',\n",
       " 185: 'leaves',\n",
       " 186: 'zone',\n",
       " 187: 'purpose',\n",
       " 188: 'facilitate',\n",
       " 189: 'eur235',\n",
       " 190: 'eoss',\n",
       " 191: 'reserved',\n",
       " 192: 'flock',\n",
       " 193: 'bofa',\n",
       " 194: 'sparks',\n",
       " 195: 'larox',\n",
       " 196: 'dramatically',\n",
       " 197: 'metric',\n",
       " 198: '5343',\n",
       " 199: 'proves',\n",
       " 200: 'making',\n",
       " 201: 'sek150',\n",
       " 202: 'higherend',\n",
       " 203: 'effects',\n",
       " 204: 'pdas',\n",
       " 205: 'plus',\n",
       " 206: 'swot',\n",
       " 207: '0930',\n",
       " 208: 'monitor',\n",
       " 209: 'bia',\n",
       " 210: 'surfeit',\n",
       " 211: 'metrics',\n",
       " 212: 'heat',\n",
       " 213: 'stx',\n",
       " 214: 'layer',\n",
       " 215: 'trademarks',\n",
       " 216: 'homebase',\n",
       " 217: '1569',\n",
       " 218: 'foot',\n",
       " 219: 'mining',\n",
       " 220: 'osakeyhti¦',\n",
       " 221: 'almost',\n",
       " 222: 'determined',\n",
       " 223: 'parties',\n",
       " 224: 'haven',\n",
       " 225: '1740',\n",
       " 226: 'pecs',\n",
       " 227: 'domain',\n",
       " 228: '016',\n",
       " 229: 'sight',\n",
       " 230: 'romania',\n",
       " 231: 'simdax',\n",
       " 232: 'already',\n",
       " 233: 'joint',\n",
       " 234: 'bio',\n",
       " 235: 'metro',\n",
       " 236: 'mmh',\n",
       " 237: 'timetable',\n",
       " 238: 'redistribution',\n",
       " 239: 'pharmacokinetics',\n",
       " 240: 'qingyuan',\n",
       " 241: 'concept',\n",
       " 242: '092',\n",
       " 243: 'model',\n",
       " 244: 'weighted',\n",
       " 245: 'troubles',\n",
       " 246: 'modules',\n",
       " 247: 'kolorit',\n",
       " 248: 'pattern',\n",
       " 249: 'scamming',\n",
       " 250: 'consolidation',\n",
       " 251: 'anglers',\n",
       " 252: '111',\n",
       " 253: 't',\n",
       " 254: 'method',\n",
       " 255: 'crecent',\n",
       " 256: 'broader',\n",
       " 257: '888',\n",
       " 258: 'coagulant',\n",
       " 259: 'paijathame',\n",
       " 260: 'completely',\n",
       " 261: 'workflows',\n",
       " 262: 'bluebird',\n",
       " 263: 'course',\n",
       " 264: 'profiling',\n",
       " 265: 'measurement',\n",
       " 266: 'resulting',\n",
       " 267: 'aftermarket',\n",
       " 268: '22us',\n",
       " 269: 'hangasjarvi',\n",
       " 270: 'laboratory',\n",
       " 271: 'merrill',\n",
       " 272: 'wo',\n",
       " 273: '4608',\n",
       " 274: 'promo',\n",
       " 275: 'sheetmetal',\n",
       " 276: 'extra',\n",
       " 277: 'exel',\n",
       " 278: 'blocking',\n",
       " 279: 'shooting',\n",
       " 280: '327',\n",
       " 281: 'undertakings',\n",
       " 282: 'insurer',\n",
       " 283: '241',\n",
       " 284: 'core',\n",
       " 285: '309',\n",
       " 286: 'endowment',\n",
       " 287: '609',\n",
       " 288: 'cliffton',\n",
       " 289: 'maintained',\n",
       " 290: 'finance',\n",
       " 291: 'lansivayla',\n",
       " 292: 'queue',\n",
       " 293: 'codetermination',\n",
       " 294: 'baltimore',\n",
       " 295: 'jawad',\n",
       " 296: '354',\n",
       " 297: 'strategic',\n",
       " 298: 'lows',\n",
       " 299: '2008',\n",
       " 300: 'suomalainen',\n",
       " 301: 'alltime',\n",
       " 302: '1862',\n",
       " 303: 'blending',\n",
       " 304: 'volkswagen',\n",
       " 305: 'environmentally',\n",
       " 306: 'spark',\n",
       " 307: 'expression',\n",
       " 308: 'thin',\n",
       " 309: 'g400',\n",
       " 310: 'ferguson',\n",
       " 311: 'separately',\n",
       " 312: 'olympic',\n",
       " 313: 'adanac',\n",
       " 314: 'ntt',\n",
       " 315: 'finnlines',\n",
       " 316: 'meteorological',\n",
       " 317: 'interbank',\n",
       " 318: 'sanoma',\n",
       " 319: 'strait',\n",
       " 320: 'retailer',\n",
       " 321: 'redevelopment',\n",
       " 322: 'logset',\n",
       " 323: 'fray',\n",
       " 324: 'dvd',\n",
       " 325: 'retained',\n",
       " 326: '334£323£6201',\n",
       " 327: 'license',\n",
       " 328: 'doprava',\n",
       " 329: 'crude',\n",
       " 330: 'indepth',\n",
       " 331: 'cycle',\n",
       " 332: '377',\n",
       " 333: '247',\n",
       " 334: 'procedures',\n",
       " 335: 'dmasia16',\n",
       " 336: 'e7',\n",
       " 337: 'removable',\n",
       " 338: 'healthy',\n",
       " 339: 'reseller',\n",
       " 340: 'underweight',\n",
       " 341: 'ince',\n",
       " 342: 'wwwpeigsse',\n",
       " 343: 'banks',\n",
       " 344: 'nordic',\n",
       " 345: 'certain',\n",
       " 346: '504',\n",
       " 347: 'imaging',\n",
       " 348: 'paycheck',\n",
       " 349: 'disappear',\n",
       " 350: 'rolloff',\n",
       " 351: 'gallup',\n",
       " 352: 'bm4',\n",
       " 353: 'beverage',\n",
       " 354: 'strive',\n",
       " 355: 'beautifully',\n",
       " 356: 'bestseller',\n",
       " 357: '598',\n",
       " 358: 'letter',\n",
       " 359: 'lifecycle',\n",
       " 360: 'jams',\n",
       " 361: 'labs',\n",
       " 362: 'microwaveable',\n",
       " 363: 'hk',\n",
       " 364: 'contest',\n",
       " 365: 'evaluated',\n",
       " 366: 'mformation',\n",
       " 367: 'degridding',\n",
       " 368: 'vaias',\n",
       " 369: 'kallio',\n",
       " 370: 'usd12',\n",
       " 371: 'sewed',\n",
       " 372: '767',\n",
       " 373: 'perspective',\n",
       " 374: 'montevideo',\n",
       " 375: '20072008',\n",
       " 376: 'approximately',\n",
       " 377: 'constructionreal',\n",
       " 378: 'banken',\n",
       " 379: '138',\n",
       " 380: 'hits',\n",
       " 381: 'strong',\n",
       " 382: 'minutes',\n",
       " 383: 'timefocused',\n",
       " 384: 'sticks',\n",
       " 385: '964',\n",
       " 386: 'tournament',\n",
       " 387: '613',\n",
       " 388: 'sophos',\n",
       " 389: 'communications',\n",
       " 390: '042',\n",
       " 391: 'popular',\n",
       " 392: '371',\n",
       " 393: 'surveillance',\n",
       " 394: 'aleksandri',\n",
       " 395: 'clickhappy',\n",
       " 396: 'farmer',\n",
       " 397: 'reward',\n",
       " 398: '933',\n",
       " 399: 'tex',\n",
       " 400: 'williams',\n",
       " 401: 'conversion',\n",
       " 402: 'supervisor',\n",
       " 403: 'ground',\n",
       " 404: '13000',\n",
       " 405: 'recycling',\n",
       " 406: '19982006',\n",
       " 407: 'read',\n",
       " 408: 'jung',\n",
       " 409: 'worn',\n",
       " 410: 'exploring',\n",
       " 411: 'thereby',\n",
       " 412: 'edible',\n",
       " 413: 'rights',\n",
       " 414: 'typical',\n",
       " 415: 'vegetable',\n",
       " 416: '500000',\n",
       " 417: 'tlt1v',\n",
       " 418: 'reuse',\n",
       " 419: 'however',\n",
       " 420: 'capitel',\n",
       " 421: '2770',\n",
       " 422: 'vtmrakennuskonevuokraamo',\n",
       " 423: 'continues',\n",
       " 424: 'declined',\n",
       " 425: 'utilising',\n",
       " 426: 'matches',\n",
       " 427: 'mpra',\n",
       " 428: 'previousyear',\n",
       " 429: 'williamson',\n",
       " 430: 'smarket',\n",
       " 431: '195',\n",
       " 432: 'immediate',\n",
       " 433: 'unsecured',\n",
       " 434: 'airspace',\n",
       " 435: 'hannu',\n",
       " 436: '1152007',\n",
       " 437: 'spanning',\n",
       " 438: 'link',\n",
       " 439: 'ebusiness',\n",
       " 440: 'grupe',\n",
       " 441: 'solar',\n",
       " 442: 'games',\n",
       " 443: 'wallhamn',\n",
       " 444: 'active',\n",
       " 445: 'constructive',\n",
       " 446: 'aleksanterinkatu',\n",
       " 447: 'addition',\n",
       " 448: 'derive',\n",
       " 449: 'jouko',\n",
       " 450: 'eastern',\n",
       " 451: 'france',\n",
       " 452: 'esa',\n",
       " 453: 'sysopen',\n",
       " 454: 'features',\n",
       " 455: 'bottomline',\n",
       " 456: 'enters',\n",
       " 457: 'ahl1v',\n",
       " 458: '774',\n",
       " 459: 'intensifying',\n",
       " 460: 'customised',\n",
       " 461: '113000',\n",
       " 462: 'roman',\n",
       " 463: 'panel',\n",
       " 464: 'composites',\n",
       " 465: 'rs310',\n",
       " 466: 'vaasa',\n",
       " 467: 'coordinated',\n",
       " 468: 'calls',\n",
       " 469: 'corporations',\n",
       " 470: '6003',\n",
       " 471: 'sciences',\n",
       " 472: 'centrally',\n",
       " 473: 'onodi',\n",
       " 474: 'fight',\n",
       " 475: 'bed',\n",
       " 476: 'karputer',\n",
       " 477: 'number',\n",
       " 478: 'magazine',\n",
       " 479: 'jerusalem',\n",
       " 480: 'inaugurating',\n",
       " 481: '342',\n",
       " 482: 'jorgen',\n",
       " 483: 'archipelago',\n",
       " 484: '1519',\n",
       " 485: 'instead',\n",
       " 486: 'krakeroy',\n",
       " 487: 'bohemia',\n",
       " 488: 'bcd',\n",
       " 489: 'intel',\n",
       " 490: 'eur97m',\n",
       " 491: '236',\n",
       " 492: 'cost',\n",
       " 493: 'intentions',\n",
       " 494: 'cannedpreserved',\n",
       " 495: 'hong',\n",
       " 496: 'pda',\n",
       " 497: '21179',\n",
       " 498: 'tailormade',\n",
       " 499: 'enefit280',\n",
       " 500: '4q07',\n",
       " 501: '806',\n",
       " 502: 'euro18',\n",
       " 503: '1700',\n",
       " 504: 'õ',\n",
       " 505: 'eur1520m',\n",
       " 506: 'secured',\n",
       " 507: 'quick',\n",
       " 508: 'marchapril',\n",
       " 509: 'exceed',\n",
       " 510: 'swaps',\n",
       " 511: 'proceed',\n",
       " 512: 'chief',\n",
       " 513: '834',\n",
       " 514: 'nonbinding',\n",
       " 515: 'tammela',\n",
       " 516: 'obligated',\n",
       " 517: 'essent',\n",
       " 518: '850',\n",
       " 519: 'onroad',\n",
       " 520: 'nearby',\n",
       " 521: 'compulsory',\n",
       " 522: 'terrain',\n",
       " 523: 'infected',\n",
       " 524: 'loading',\n",
       " 525: 'dynamic',\n",
       " 526: 'eur63',\n",
       " 527: 'maruti',\n",
       " 528: 'consistent',\n",
       " 529: 'seconds',\n",
       " 530: 'honda',\n",
       " 531: '46',\n",
       " 532: 'otherwise',\n",
       " 533: 'gustav',\n",
       " 534: 'precipitation',\n",
       " 535: 'status',\n",
       " 536: 'kdg',\n",
       " 537: 'development',\n",
       " 538: 'passages',\n",
       " 539: 'verification',\n",
       " 540: 'ovako',\n",
       " 541: '3611',\n",
       " 542: 'zainalabedin',\n",
       " 543: 'valkeakoski',\n",
       " 544: 'degree',\n",
       " 545: 'outoteccom',\n",
       " 546: 'viking',\n",
       " 547: 'turnover',\n",
       " 548: 'baltics',\n",
       " 549: 'cross',\n",
       " 550: 'alandsbanken',\n",
       " 551: 'elqav',\n",
       " 552: 'hankkijamaatalous',\n",
       " 553: '20112015',\n",
       " 554: 'selloff',\n",
       " 555: 'classroom',\n",
       " 556: 'dirk',\n",
       " 557: '5401',\n",
       " 558: 'scissors',\n",
       " 559: 'pekka',\n",
       " 560: 'mtyear',\n",
       " 561: 'eloholma',\n",
       " 562: 'technologyoriented',\n",
       " 563: 'happy',\n",
       " 564: 'conspiracy',\n",
       " 565: 'neareast',\n",
       " 566: 'served',\n",
       " 567: 'hautaniemi',\n",
       " 568: 'periodend',\n",
       " 569: 'gunpoint',\n",
       " 570: 'verizon',\n",
       " 571: 'lasse',\n",
       " 572: 'sheds',\n",
       " 573: 'poultry',\n",
       " 574: 'further',\n",
       " 575: 'rollon',\n",
       " 576: 'nevsky',\n",
       " 577: 'stations',\n",
       " 578: 'recalling',\n",
       " 579: 'species',\n",
       " 580: 'master',\n",
       " 581: '530773',\n",
       " 582: 'sihvonen',\n",
       " 583: 'sveikatos',\n",
       " 584: 'riihimæki',\n",
       " 585: 'major',\n",
       " 586: 'h1',\n",
       " 587: 'pacts',\n",
       " 588: 'operations',\n",
       " 589: 'relative',\n",
       " 590: 'join',\n",
       " 591: 'talk',\n",
       " 592: 'microelectronic',\n",
       " 593: 'burns',\n",
       " 594: 'mcdonald',\n",
       " 595: 'summary',\n",
       " 596: 'guard',\n",
       " 597: 'acquisitions',\n",
       " 598: 'now',\n",
       " 599: 'chuck',\n",
       " 600: 'brokerage',\n",
       " 601: 'diligence',\n",
       " 602: '00',\n",
       " 603: 'interoperability',\n",
       " 604: 'distribute',\n",
       " 605: 'grannan',\n",
       " 606: 'mentions',\n",
       " 607: 'tanks',\n",
       " 608: 'ra',\n",
       " 609: 'priority',\n",
       " 610: 'liter',\n",
       " 611: 'trading',\n",
       " 612: '19',\n",
       " 613: '400',\n",
       " 614: 'boycotting',\n",
       " 615: 'inventor',\n",
       " 616: 'paperboard',\n",
       " 617: 'ministerija',\n",
       " 618: '575000',\n",
       " 619: 'melngailis',\n",
       " 620: 'onestopshop',\n",
       " 621: 'hkscan',\n",
       " 622: 'renewal',\n",
       " 623: '485',\n",
       " 624: 'royal',\n",
       " 625: 'builders',\n",
       " 626: 'municipality',\n",
       " 627: 'rpm',\n",
       " 628: 'ænekoski',\n",
       " 629: 'teenagers',\n",
       " 630: 'kci',\n",
       " 631: 'kia',\n",
       " 632: 'telekom',\n",
       " 633: 'report17',\n",
       " 634: '825mn',\n",
       " 635: 'telco',\n",
       " 636: 'plains',\n",
       " 637: 'scrap',\n",
       " 638: 'totalled',\n",
       " 639: 'smoothly',\n",
       " 640: 'require',\n",
       " 641: 'loose',\n",
       " 642: 'fradkov',\n",
       " 643: 'pivo',\n",
       " 644: 'geographic',\n",
       " 645: '7day',\n",
       " 646: 'idea',\n",
       " 647: 'amendment',\n",
       " 648: 'amid',\n",
       " 649: 'compagnie',\n",
       " 650: 'our',\n",
       " 651: '501percent',\n",
       " 652: 'legacy',\n",
       " 653: '2346',\n",
       " 654: 'needed',\n",
       " 655: '131',\n",
       " 656: 'periods',\n",
       " 657: 'liquefied',\n",
       " 658: '31',\n",
       " 659: 'designing',\n",
       " 660: 'icelandair',\n",
       " 661: 'therapeutic',\n",
       " 662: '3year',\n",
       " 663: '1230',\n",
       " 664: 'gone',\n",
       " 665: 'raivv',\n",
       " 666: 'recari',\n",
       " 667: '5204',\n",
       " 668: 'maritime',\n",
       " 669: 'sidewalks',\n",
       " 670: 'foremost',\n",
       " 671: 'platinum',\n",
       " 672: 'kaupthing',\n",
       " 673: 'television',\n",
       " 674: 'vashi',\n",
       " 675: 'refurbishing',\n",
       " 676: 'tom',\n",
       " 677: 'producer',\n",
       " 678: 'blond',\n",
       " 679: 'collar',\n",
       " 680: 'secondary',\n",
       " 681: 'significant',\n",
       " 682: 'falls',\n",
       " 683: 'late',\n",
       " 684: '320000',\n",
       " 685: 'bilbao',\n",
       " 686: 'retail',\n",
       " 687: 'keskuskatu',\n",
       " 688: 'ship',\n",
       " 689: 'signing',\n",
       " 690: 'goodyear',\n",
       " 691: 'liquid',\n",
       " 692: 'redeem',\n",
       " 693: 'reduces',\n",
       " 694: '330000',\n",
       " 695: '040',\n",
       " 696: 'recalls',\n",
       " 697: '24592',\n",
       " 698: 'lighter',\n",
       " 699: 'bodies',\n",
       " 700: 'roro',\n",
       " 701: 'electrically',\n",
       " 702: 'employees',\n",
       " 703: 'geo1v',\n",
       " 704: 'annvik',\n",
       " 705: 'tape',\n",
       " 706: 'actually',\n",
       " 707: 'tapani',\n",
       " 708: 'brian',\n",
       " 709: 'challenge',\n",
       " 710: '78017',\n",
       " 711: 'hyunchang',\n",
       " 712: 'pay',\n",
       " 713: 'updates',\n",
       " 714: 'experiencing',\n",
       " 715: '543',\n",
       " 716: 'papers',\n",
       " 717: 'reporters',\n",
       " 718: 'xns1v',\n",
       " 719: 'seminars',\n",
       " 720: 'rotation',\n",
       " 721: 'developer',\n",
       " 722: 'less',\n",
       " 723: 'shale',\n",
       " 724: 'gabbana',\n",
       " 725: 'hintikka',\n",
       " 726: 'inkinen',\n",
       " 727: 'remained',\n",
       " 728: 'dhabi',\n",
       " 729: 'campaign',\n",
       " 730: 'rent',\n",
       " 731: 'gas',\n",
       " 732: 'tutkimuksen',\n",
       " 733: 'vendors',\n",
       " 734: 'harbor',\n",
       " 735: 'handpainted',\n",
       " 736: 'peri',\n",
       " 737: 'continuing',\n",
       " 738: '3d',\n",
       " 739: 'figure',\n",
       " 740: 'avc',\n",
       " 741: '20042005',\n",
       " 742: 'rostelecom',\n",
       " 743: 'highclass',\n",
       " 744: 'salaried',\n",
       " 745: 'noop',\n",
       " 746: 'findest',\n",
       " 747: 'ylinen',\n",
       " 748: 'dyed',\n",
       " 749: '191',\n",
       " 750: 'tver',\n",
       " 751: 'charlotte',\n",
       " 752: 'builds',\n",
       " 753: 'altra',\n",
       " 754: 'refuted',\n",
       " 755: 'fantuzzi',\n",
       " 756: 'threeday',\n",
       " 757: 'stora',\n",
       " 758: 'treatments',\n",
       " 759: '9200',\n",
       " 760: 'uruguayan',\n",
       " 761: '615',\n",
       " 762: 'evraz',\n",
       " 763: 'midmarket',\n",
       " 764: 'designated',\n",
       " 765: 'hang',\n",
       " 766: 'ccs',\n",
       " 767: '10113',\n",
       " 768: 're',\n",
       " 769: 'ondemand',\n",
       " 770: 'managing',\n",
       " 771: 'fit',\n",
       " 772: 'networking',\n",
       " 773: 'cet',\n",
       " 774: '29659239',\n",
       " 775: 'forssan',\n",
       " 776: 'intends',\n",
       " 777: 'hired',\n",
       " 778: 'synonymous',\n",
       " 779: 'schultz',\n",
       " 780: 'cinema',\n",
       " 781: 'anne',\n",
       " 782: 'hameenlinna',\n",
       " 783: 'laamanen',\n",
       " 784: '10706',\n",
       " 785: 'fivestorey',\n",
       " 786: 'patents',\n",
       " 787: 'sections',\n",
       " 788: 'leningrad',\n",
       " 789: 'lovely',\n",
       " 790: 'administrators',\n",
       " 791: 'frontedge',\n",
       " 792: 'secondbiggest',\n",
       " 793: 'relations',\n",
       " 794: 'tiein',\n",
       " 795: 'swoope',\n",
       " 796: 'steve',\n",
       " 797: 'sir',\n",
       " 798: '64',\n",
       " 799: 'eng',\n",
       " 800: 'abilities',\n",
       " 801: 'bergqvist',\n",
       " 802: 'ict',\n",
       " 803: 'extremely',\n",
       " 804: 'remotely',\n",
       " 805: 'mentioned',\n",
       " 806: 'sponsors',\n",
       " 807: 'winner',\n",
       " 808: 'compression',\n",
       " 809: 'proposition',\n",
       " 810: 'budget',\n",
       " 811: 'pool',\n",
       " 812: 'using',\n",
       " 813: 'bjorn',\n",
       " 814: 'capitals',\n",
       " 815: 'were',\n",
       " 816: 'revise',\n",
       " 817: 'scanning',\n",
       " 818: 'onethird',\n",
       " 819: '486',\n",
       " 820: '608',\n",
       " 821: '01011385',\n",
       " 822: 'guide',\n",
       " 823: 'asbuilt',\n",
       " 824: 'indicated',\n",
       " 825: 'orhangazi',\n",
       " 826: 'involve',\n",
       " 827: 'veil',\n",
       " 828: 'hospitals',\n",
       " 829: 'unique',\n",
       " 830: 'shoe',\n",
       " 831: 'remain',\n",
       " 832: 'posted',\n",
       " 833: '1992',\n",
       " 834: 'amanda',\n",
       " 835: 'tikkakoski',\n",
       " 836: 'innovationsmanagement',\n",
       " 837: 'unit',\n",
       " 838: '1007',\n",
       " 839: 'members',\n",
       " 840: 'states',\n",
       " 841: '800dwt',\n",
       " 842: 'gains',\n",
       " 843: 'discuss',\n",
       " 844: 'kolkhoznitsa',\n",
       " 845: 'ltda',\n",
       " 846: 'forwarders',\n",
       " 847: '88',\n",
       " 848: 'vinachem',\n",
       " 849: 'reclaimed',\n",
       " 850: 'technopolis',\n",
       " 851: 'wwwfiskarscom',\n",
       " 852: 'radar',\n",
       " 853: '2216',\n",
       " 854: 'mediaphones',\n",
       " 855: 'tank',\n",
       " 856: 'measure',\n",
       " 857: 'loudeye',\n",
       " 858: 'telia',\n",
       " 859: 'invitation',\n",
       " 860: 'speed',\n",
       " 861: 'adak',\n",
       " 862: 'brewerton',\n",
       " 863: 'talking',\n",
       " 864: 'friends',\n",
       " 865: 'philadelphia',\n",
       " 866: 'eur3',\n",
       " 867: 'locator',\n",
       " 868: 'julia',\n",
       " 869: 'rop',\n",
       " 870: 'protect',\n",
       " 871: 'hoyer',\n",
       " 872: 'tabloid',\n",
       " 873: 'mega',\n",
       " 874: 'lunch',\n",
       " 875: 'spinoff',\n",
       " 876: '753',\n",
       " 877: '1987',\n",
       " 878: 'crushing',\n",
       " 879: 'intermittently',\n",
       " 880: 'universal',\n",
       " 881: 'standardised',\n",
       " 882: '2541',\n",
       " 883: 'estimates',\n",
       " 884: 'multiplayer',\n",
       " 885: 'disappeared',\n",
       " 886: 'storage',\n",
       " 887: 'best',\n",
       " 888: 'put',\n",
       " 889: 'interactions',\n",
       " 890: 'mc3090',\n",
       " 891: '8000',\n",
       " 892: 'matinkyla',\n",
       " 893: '2924',\n",
       " 894: 'russia',\n",
       " 895: 'lemminkainen',\n",
       " 896: 'wwwstockmanncom',\n",
       " 897: 'dragged',\n",
       " 898: 'sda1v',\n",
       " 899: 'sciard',\n",
       " 900: 'dyes',\n",
       " 901: 'czech',\n",
       " 902: 'repayments',\n",
       " 903: 'conveniences',\n",
       " 904: 'traces',\n",
       " 905: 'integration',\n",
       " 906: 'resignation',\n",
       " 907: 'ones',\n",
       " 908: '26',\n",
       " 909: 'category',\n",
       " 910: 'since',\n",
       " 911: 'sectors',\n",
       " 912: 'salcomp',\n",
       " 913: 'januarymay',\n",
       " 914: 'norske',\n",
       " 915: 'expects',\n",
       " 916: 'travellers',\n",
       " 917: 'heart',\n",
       " 918: 'misleading',\n",
       " 919: 'enterprise',\n",
       " 920: '8232',\n",
       " 921: '267mn',\n",
       " 922: 'failure',\n",
       " 923: 'grinding',\n",
       " 924: 'pensioenfonds',\n",
       " 925: 'monthly',\n",
       " 926: 'transport',\n",
       " 927: 'ugglarp',\n",
       " 928: 'client',\n",
       " 929: 'sastamala',\n",
       " 930: 'heads',\n",
       " 931: 'subscriber',\n",
       " 932: 'eur43',\n",
       " 933: 'hkan',\n",
       " 934: '24hour',\n",
       " 935: '262',\n",
       " 936: 'twig',\n",
       " 937: 'carried',\n",
       " 938: 'personalized',\n",
       " 939: 'altimo',\n",
       " 940: 'listed',\n",
       " 941: 'decoration',\n",
       " 942: '181',\n",
       " 943: 'c',\n",
       " 944: 'erection',\n",
       " 945: '1649',\n",
       " 946: 'formadehyde',\n",
       " 947: '2595',\n",
       " 948: 'brno',\n",
       " 949: 'salminen',\n",
       " 950: 'stuck',\n",
       " 951: 'maijaliisa',\n",
       " 952: 'stole',\n",
       " 953: 'crossing',\n",
       " 954: 'mid2009',\n",
       " 955: 'also',\n",
       " 956: 'shareholder',\n",
       " 957: 'euro172',\n",
       " 958: 'stateoftheart',\n",
       " 959: 'fullyowned',\n",
       " 960: '813191',\n",
       " 961: 'publ',\n",
       " 962: 'marta',\n",
       " 963: 'repo',\n",
       " 964: 'laborious',\n",
       " 965: '10year',\n",
       " 966: 'retrofit',\n",
       " 967: 'eur30',\n",
       " 968: 'adidas',\n",
       " 969: 'china',\n",
       " 970: 'narrow',\n",
       " 971: 'glassfiber',\n",
       " 972: 'requesting',\n",
       " 973: 'halonen',\n",
       " 974: '036',\n",
       " 975: 'indicate',\n",
       " 976: 'hallein',\n",
       " 977: 'specialises',\n",
       " 978: 'flies',\n",
       " 979: 'tonne',\n",
       " 980: 'uses',\n",
       " 981: 'eur107',\n",
       " 982: 'extensive',\n",
       " 983: '808',\n",
       " 984: 'incorporates',\n",
       " 985: 'tidningar',\n",
       " 986: '1415',\n",
       " 987: 'conservative',\n",
       " 988: 'twh',\n",
       " 989: '14',\n",
       " 990: 'carbon',\n",
       " 991: 'rmg',\n",
       " 992: 'acacia',\n",
       " 993: 'digital',\n",
       " 994: 'mind',\n",
       " 995: 'autonomous',\n",
       " 996: 'ostroleka',\n",
       " 997: '26032010',\n",
       " 998: 'kpy',\n",
       " 999: 'userexperience',\n",
       " 1000: 'turnaround',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dictionaries(words):\n",
    "    word_to_int_dict = {w:i+1 for i, w in enumerate(words)}\n",
    "    int_to_word_dict = {i:w for w, i in word_to_int_dict.items()}\n",
    "    return word_to_int_dict, int_to_word_dict\n",
    "\n",
    "word_to_int_dict, int_to_word_dict = create_dictionaries(vocab)\n",
    "\n",
    "int_to_word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word_to_int_dict_financial.json', 'w') as fp:\n",
    "    json.dump(word_to_int_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "20.49422203879488\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(x) for x in reviews]))\n",
    "print(np.mean([len(x) for x in reviews]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '',\n",
       "       '', '', '', '', '', '', '', '', '', '', '', 'according', 'to',\n",
       "       'gran', 'the', 'company', 'has', 'no', 'plans', 'to', 'move',\n",
       "       'all', 'production', 'to', 'russia', 'although', 'that', 'is',\n",
       "       'where', 'the', 'company', 'is', 'growing'], dtype='<U25')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_text(tokenized_reviews, seq_length):\n",
    "    \n",
    "    reviews = []\n",
    "    \n",
    "    for review in tokenized_reviews:\n",
    "        if len(review) >= seq_length:\n",
    "            reviews.append(review[:seq_length])\n",
    "        else:\n",
    "            reviews.append(['']*(seq_length-len(review)) + review)\n",
    "        \n",
    "    return np.array(reviews)\n",
    "\n",
    "padded_sentences = pad_text(reviews, seq_length = 50)\n",
    "\n",
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_word_dict[0] = ''\n",
    "word_to_int_dict[''] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,  9302,  3832,  6230,  8349,  9130,  1526, 10779,  6582,\n",
       "        3832,  8453,  9223,  3805,  3832,   894,  5309,  6047,  7924,\n",
       "        2516,  8349,  9130,  7924,  1731])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sentences = np.array([[word_to_int_dict[word] for word in review] for review in padded_sentences])\n",
    "\n",
    "encoded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_vocab = n_vocab  \n",
    "        self.n_layers = n_layers \n",
    "        self.n_hidden = n_hidden \n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                          \n",
    "        embedded_words = self.embedding(input_words)\n",
    "        lstm_out, h = self.lstm(embedded_words) \n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                  \n",
    "        sigmoid_out = self.sigmoid(fc_out)              \n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  \n",
    "        \n",
    "        sigmoid_last = sigmoid_out[:, -1]\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):\n",
    "        \n",
    "        device = \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(word_to_int_dict)\n",
    "n_embed = 50\n",
    "n_hidden = 100\n",
    "n_output = 1\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_encoding(data):\n",
    "#     labels= np.empty([data.shape[0]])\n",
    "    \n",
    "#     for i in range(data.shape[0]):\n",
    "#         if data['Label'][i]== 'negative':\n",
    "#             labels[i]= 0\n",
    "#         elif data['Label'][i]== 'neutral':\n",
    "#             labels[i]= 1\n",
    "#         else:\n",
    "#             labels[i]= 2\n",
    "            \n",
    "#     return labels\n",
    "\n",
    "# labels= label_encoding(data)\n",
    "\n",
    "# labels.to_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_arr= np.array(labels)\n",
    "# type(labels_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([int(x) for x in data['Label'].values])\n",
    "# labels= label_encoding(data)\n",
    "\n",
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "\n",
    "total = len(encoded_sentences)\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = torch.Tensor(encoded_sentences[:train_cutoff]).long(), torch.Tensor(labels[:train_cutoff]).long()\n",
    "valid_x, valid_y = torch.Tensor(encoded_sentences[train_cutoff : valid_cutoff]).long(), torch.Tensor(labels[train_cutoff : valid_cutoff]).long()\n",
    "test_x, test_y = torch.Tensor(encoded_sentences[valid_cutoff:]).long(), torch.Tensor(labels[valid_cutoff:])\n",
    "\n",
    "train_data = TensorDataset(train_x, train_y)\n",
    "valid_data = TensorDataset(valid_x, valid_y)\n",
    "test_data = TensorDataset(test_x, test_y)\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 2400\n",
    "step = 0\n",
    "n_epochs = 3\n",
    "clip = 5  \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12176/2499155895.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLP2201\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLP2201\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\NLP2201\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2904\u001b[0m         \u001b[0mreduction_enum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;34m\"Using a target size ({}) that is different to the input size ({}) is deprecated. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m             \u001b[1;34m\"Please ensure they have the same size.\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    labels= labels.unsqueeze(1)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss = criterion(output.squeeze(), labels())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paul_\\AppData\\Local\\Temp/ipykernel_12176/2906406983.py:10: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3 Step: 2400 Training Loss: 0.0000 Validation Loss: 26.5979\n",
      "Epoch: 2/3 Step: 4800 Training Loss: 0.0000 Validation Loss: 26.5979\n",
      "Epoch: 2/3 Step: 7200 Training Loss: 0.0000 Validation Loss: 26.5979\n",
      "Epoch: 3/3 Step: 9600 Training Loss: 0.0000 Validation Loss: 26.5979\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        step += 1  \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.squeeze().float())\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "\n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                       \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.squeeze().float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_fin.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(v_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.load_state_dict(torch.load('model_fin.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 58.5567\n",
      "Test Accuracy: 0.20\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    test_output, test_h = net(inputs)\n",
    "    loss = criterion(test_output, labels)\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_review(review):\n",
    "    review = review.translate(str.maketrans('', '', punctuation)).lower().rstrip()\n",
    "    tokenized = word_tokenize(review)\n",
    "    if len(tokenized) >= 50:\n",
    "        review = tokenized[:50]\n",
    "    else:\n",
    "        review= ['0']*(50-len(tokenized)) + tokenized\n",
    "    \n",
    "    final = []\n",
    "    \n",
    "    for token in review:\n",
    "        try:\n",
    "            final.append(word_to_int_dict[token])\n",
    "            \n",
    "        except:\n",
    "            final.append(word_to_int_dict[''])\n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(review):\n",
    "    net.eval()\n",
    "    words = np.array([preprocess_review(review)])\n",
    "    padded_words = torch.from_numpy(words)\n",
    "    pred_loader = DataLoader(padded_words, batch_size = 1, shuffle = True)\n",
    "    for x in pred_loader:\n",
    "        output = net(x)[0].item()\n",
    "    \n",
    "    msg = \"This is a positive review.\" if output >= 0.5 else \"This is a negative review.\"\n",
    "    print(msg)\n",
    "    print('Prediction = ' + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive review.\n",
      "Prediction = 1.0\n"
     ]
    }
   ],
   "source": [
    "predict(\"Tesla is doing very bad. But elon seems to be worried with the war. Lets see what happens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a positive review.\n",
      "Prediction = 1.0\n"
     ]
    }
   ],
   "source": [
    "predict(\"It was not good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
